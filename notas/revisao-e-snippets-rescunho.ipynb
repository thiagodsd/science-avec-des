{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7I-pizlmYGDz"
   },
   "source": [
    "# Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CX3-taG6tmHf"
   },
   "source": [
    "\n",
    "\n",
    "\n",
    "```python\n",
    "import numpy  as np\n",
    "import pandas as pd\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mHemfsKVf3SF"
   },
   "source": [
    "## Classical Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Pg8EyLNzgPwk"
   },
   "source": [
    "### Supervised"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KBKidjQfklvT"
   },
   "source": [
    "#### Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "07PTAU6Pknrd"
   },
   "source": [
    "#### Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "P-AY8hHYoNnT"
   },
   "source": [
    "#### SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6XPPa_C3pHFe"
   },
   "source": [
    "IMPORTANTISSIMO: a `cross_val_score` retorna o score no **teste** sempre! Entãão algo como\n",
    "\n",
    "```\n",
    "scr_f1 = cross_val_score(SVC(kernel='linear', C=1), df.drop(labels=['target'], axis=1), df['target'], cv=10, scoring='f1')\n",
    "\n",
    "display(scr_f1)\n",
    "display(np.mean(scr_f1))\n",
    "```\n",
    "\n",
    "Vai dar problema se uma questao pede score no **treino**. Um jeito mais geral de fazer o cross-validation esta escrito abaixo:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cWktHHe-oPXE"
   },
   "source": [
    "```python\n",
    "from sklearn.svm             import SVC\n",
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "from sklearn.metrics         import f1_score\n",
    "\n",
    "df = pd.read_csv('{}/itub/Questoes/classificacao_1.csv'.format(PATH))\n",
    "df.head()\n",
    "\n",
    "scores = list()\n",
    "\n",
    "X = df.drop(labels=['target'], inplace=False, axis=1)\n",
    "y = df['target']\n",
    "\n",
    "for train_id, test_id in KFold(n_splits=10).split(df):\n",
    "  X_train, X_test = X.iloc[train_id], X.iloc[test_id]\n",
    "  y_train, y_test = y.iloc[train_id], y.iloc[test_id]\n",
    "\n",
    "  svc = SVC(kernel='linear', C=1)\n",
    "  svc.fit(X_train, y_train)\n",
    "  \n",
    "  # NAS PARTICOES DE TREINO !\n",
    "  # y_pred = svc.predict(X_test)\n",
    "  # scores.append(f1_score(y_test, y_pred))\n",
    "\n",
    "  y_pred = svc.predict(X_train)\n",
    "  scores.append(f1_score(y_train, y_pred))\n",
    "\n",
    "display(scores)\n",
    "display(np.mean(scores))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GqwqFsb1gP2g"
   },
   "source": [
    "### Unsupervised"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "B8reHXlegP7c"
   },
   "source": [
    "#### Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8m681AMWYxto"
   },
   "source": [
    "##### Hierarchical Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XMlAhVzuY2zS"
   },
   "source": [
    "Strategies for hierarchical clustering generally fall into two types:\n",
    "\n",
    "**Agglomerative**, a \"bottom-up\" approach: each observation starts in its own cluster, and pairs of clusters are merged as one moves up the hierarchy. **Divisive**, a \"top-down\" approach: all observations start in one cluster, and splits are performed recursively as one moves down the hierarchy [ref](https://www.kaggle.com/vipulgandhi/hierarchical-clustering-explanation).\n",
    "\n",
    "Agglomerative clustering possuem algumas formas diferentes de medir dissimilaridade entre grupos, e isso define a variante do metodo.<br/>\n",
    "**single link** onde a distancia minima entre os membros de cada par de grupos e a medida similaridade; capaz de separar coisas nao-elipticas, mas fragil com respeito a ruido entre clusters.<br/>\n",
    "**complete link** em que a distancia maxima entre os membros e a medida de dissimilaridade; lida bem com ruido, mas ha um vies globular nessa abordagem, alem da tendencia de quebrar grandes clusters.<br/>\n",
    "**average link** em que a distancias entre os centroides e a medida de dissimilaridade; lida bem com ruido, mas tambem possui vies globular [ref](https://towardsdatascience.com/understanding-the-concept-of-hierarchical-clustering-technique-c6e8243758ec).\n",
    "\n",
    "Do Divisive clustering surge o **bisecting K-Means**, em que sucessivos 2-Means sao aplicados em cada cluster maior.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kYWEUy8pmS5-"
   },
   "source": [
    "##### K-means\n",
    "\n",
    "[ref](https://www.kaggle.com/vipulgandhi/kmeans-detailed-explanation) com detalhes.\n",
    "\n",
    "* K-Medoids\n",
    "\n",
    "Em geral K-means usa a squared euclidian distance -- provavelmente para evitar o calculo da raiz quadrada. Isso torna o K-means menos robusto a outliers. A proposta do K-Medoids e corrigir isso. A ideia central e encontrar objetos representativos no conjunto de dados [ref](https://www.youtube.com/watch?v=GApaAnGx3Fw). Uma vez definidos os objetos representativos -- medoids -- a atualizacao acontece pela verificacao se algum outro objeto diminui a soma de medidas de similaridade. E mais caro que o K-Means, porem mais robusto.\n",
    "\n",
    "* K-Means++\n",
    "\n",
    "Inicializacoes ruins geram agrupamentos ruins no K-means -- alem de eventualmente aumentar o tempo necessario para convergencia --, entao o K-Means++ tenta corrigir isso, gerando inicializacoes quasi-otimas. A ideia consiste em escolher o primeiro centroide aleatoriamente entre os pontos a serem agrupados e a partir do segundo centroide o posicionamento e feito aleatoriamente de acordo com a probabilidade proporcional ao quadrado da distancia dos centroides existentes. Isso garante que **os centroides sejam inicializados de modo mais distante entre si**. [ref](https://medium.com/machine-learning-algorithms-from-scratch/k-means-clustering-from-scratch-in-python-1675d38eee42)\n",
    "\n",
    "Com respeito a outliers, qualquer metodo de inicializacao e otimizacao vai ser sensivel a outliers dado que a funcao objetivo e a soma de quadrados.\n",
    "\n",
    "* Mini Batch K-Means\n",
    "\n",
    "Mais rapido.\n",
    "\n",
    "* Streaming K-Means\n",
    "\n",
    "Versao online do K-means -- e portanto analoga ao Mini Batch K-Means -- em que novos dados vao surgindo e a partir dos quais a posicao dos clusteres vai sendo atualizadas. Eventualmente e possivel usar hiper-parametros para diminuir a importancia de dados muito antigos alem de ser possivel remover clusteres nao atualizados ha muito tempo. [ref](https://stats.stackexchange.com/questions/222235/explain-streaming-k-means)\n",
    "\n",
    "* K-means vs K-medians\n",
    "\n",
    "Usa mediana no lugar da raiz da distancia euclidiana, entao troca uma metrica $\\ell^2$ por uma $\\ell$.\n",
    "\n",
    "* Elbow vs Silhoutte\n",
    "\n",
    "**Elbow**: Inertia $\\times$ k-esimo cluster.<br/> \n",
    "**Silhouette**: The silhouette value measures how similar a point is to its own cluster (cohesion) compared to other clusters (separation) [ref](https://medium.com/analytics-vidhya/how-to-determine-the-optimal-k-for-k-means-708505d204eb). \n",
    "\n",
    "![](https://wikimedia.org/api/rest_v1/media/math/render/svg/3d80ab22fb291b347b2d9dc3cc7cd614f6b15479)\n",
    "\n",
    "onde $a(i)$ e a media entre i e todos os pontos, representando quao bem o i-esimo ponto esta associado **ao seu cluster**; $b(i)$ e a menor distancia entre e todos os outros pontos **dos outros clusters**, representando a dissimilaridade.\n",
    "\n",
    "Em particular para o caso de GMM a BIC pode ser uma metrica mais adequada para escolha do numero otimo de clusters.\n",
    "\n",
    "* Inertia vs Distortion\n",
    "\n",
    "**Inertia** e a soma das distancias quadraticas entre as amostras o cluster mais proximo. **Distortion** e a media das distancias quadraticas dos centroides aos elementos associados a eles.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mA48BUl8mVHC"
   },
   "source": [
    "- - -\n",
    "\n",
    "Snippet:\n",
    "\n",
    "```python\n",
    "from sklearn.cluster import KMeans, MiniBatchKMeans\n",
    "\n",
    "agr      = pd.read_csv('{}/itub/Questoes/agrupamento.csv'.format(PATH))\n",
    "centroid = pd.read_csv('{}/itub/Questoes/centroides_iniciais.csv'.format(PATH))\n",
    "\n",
    "display(agr.head())\n",
    "display(agr.shape)\n",
    "display(centroid.head())\n",
    "\n",
    "# elbow method setting cluster centroids\n",
    "\n",
    "inertia = list()\n",
    "for k in range(1, 16):\n",
    "  kmeans = KMeans(n_clusters = k)\n",
    "  kmeans.cluster_centers_ = centroid.iloc[:k ,:].as_matrix()\n",
    "  kmeans.fit(agr)\n",
    "  inertia.append(kmeans.inertia_)\n",
    "\n",
    "plt.plot([i for i in range(1,16)], inertia, '--.')\n",
    "\n",
    "# listing inertia\n",
    "\n",
    "for i in range(1,16):\n",
    "  print(i, inertia[i-1])\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "d9gWT-xF14uD"
   },
   "source": [
    "##### Mean Shift\n",
    "\n",
    "[ref](https://dashee87.github.io/data%20science/general/Clustering-with-Scikit-with-GIFs/), [ref](http://efavdb.com/mean-shift/) com mais detalhes sobre a regra de atualizacao dos pontos, e [ref](https://spin.atomicobject.com/2015/05/26/mean-shift-clustering/) com aplicacao em image segmentation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yTbPnJUPgQAB"
   },
   "source": [
    "#### Pattern Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3LZqt5p5gQEq"
   },
   "source": [
    "#### Dimension Reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5Rp1bHmQf4cQ"
   },
   "source": [
    "## Ensemble Methods\n",
    "\n",
    "Em geral ensemble learning e a aprendizagem a partir da forma de uma combinacao linear de modelos base \n",
    "\n",
    "$$f(y|x,\\pi) = \\sum_{m \\in \\mathcal{M}} w_m f_m(y|x)$$\n",
    "\n",
    "Observacoes:\n",
    "1. ha uma relacao funcional bem clara entre ensemble learning e adaptive basis-function learning, portanto **boosting tambem podem ser pensandos como ensemble learning** em que os pesos sao determinados sequencialmente, uma vez que boosting podem ser pensados como uma forma de fittar adaptives basis-function models de forma greedy. \n",
    "\n",
    "$$f(x) = w_0 + \\sum_m^M w_m \\phi_m (x)$$\n",
    "\n",
    "2. ha uma relacao aparente entre ensemble learning e **neural networks**, em que $f_m$ e analogo a m-esima camada oculta e $w_m$ sao os pesos das camadas output.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2UZofUAblsjV"
   },
   "source": [
    "### Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PToWcpkBlwIh"
   },
   "source": [
    "### Stacking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XR7wOXu0lygJ"
   },
   "source": [
    "### Bagging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dkcwBmt4f42q"
   },
   "source": [
    "## Reinforcement Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vuDyQ39Tf5Qa"
   },
   "source": [
    "## Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jThrEQXff5Yq"
   },
   "source": [
    "- - -\n",
    "\n",
    "# In-Depth Topics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "o9UIk1QJuQrq"
   },
   "source": [
    "## Bias-Variance Tradeoff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ABNo-72ff680"
   },
   "source": [
    "\n",
    "## Outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cH37167Yf7Lw"
   },
   "source": [
    "## Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "j2wQgpxUf7Vb"
   },
   "source": [
    "Cross-Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Gj6gDVsZf7gk"
   },
   "source": [
    "## Cost vs Loss vs Objective"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1_-GbTpG02PR"
   },
   "source": [
    "+ **Loss function** is usually a function **defined on a data point**, prediction and label, and measures the penalty. Ex: square loss @ linear regression.\n",
    "\n",
    "+ **Cost function** is usually more general. It might be a **sum of loss functions over your training** set plus some model complexity penalty (regularization). Ex: mse @ linear regression\n",
    "\n",
    "+ **Objective function** is the most general term for **any function that you optimize during training**. Ex: mle @ linear regression \n",
    "\n",
    "[ref](https://stats.stackexchange.com/questions/179026/objective-function-cost-function-loss-function-are-they-the-same-thing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "deoTThFSf790"
   },
   "source": [
    "## Distance Metrics\n",
    "\n",
    "![](https://raw.githubusercontent.com/thiagodsd/thiagodsd.github.io/master/img/index.png)\n",
    "\n",
    "+ [Hamming distance](https://en.wikipedia.org/wiki/Hamming_distance)\n",
    "+ [Levenshtein distance](https://en.wikipedia.org/wiki/Levenshtein_distance)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "revisao-e-snippets.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
