{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pJ-n8CnNnHfg"
   },
   "source": [
    "# 1. logistic regression and common classification issues"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8niBL0xHIOal"
   },
   "source": [
    "**odds** $$\\frac{\\text{event}}{\\text{not event}} = \\frac{p}{1-p} \\in [0, \\infty)$$\n",
    "\n",
    "**log odds**, logit $$log \\left(\\frac{p}{1-p} \\right) \\in \\mathbb{R}$$\n",
    "\n",
    "<br/>\n",
    "\n",
    "Probabilisticamente quase sempre há uma $p(y|x,w)$ que dá origem a variavel resposta, em particular\n",
    "\n",
    "1. $p \\sim \\mathcal{N}(y|w^T \\phi(x), \\sigma^2)$, regressao\n",
    "2. $p \\sim Bern\\left( y | \\frac{1}{1+e^{-w^T x}} \\right) = Bern\\left( y | \\sigma(w^T x) \\right)$, regressao logistica\n",
    "\n",
    "Relembrando que para $p(n) \\sim Bern$ vale\n",
    "\n",
    "+ $(p) \\, \\text{para} \\, n=1$\n",
    "+ $(1-p) \\, \\text{para} \\, n=0$\n",
    " \n",
    "Para o caso binário para a regressão logística, por exemplo\n",
    "\n",
    "+ $p_+ = \\sigma(w^T x)$\n",
    "+ $p_- = 1-\\sigma(w^T x) = \\sigma(-w^T x)$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SpdotZFupU8v"
   },
   "source": [
    "## fitting\n",
    "\n",
    "$$ L(w) = \\prod_i^N (\\sigma(z^i))^{y^i}(1-\\sigma(z^i))^{1-y^i}$$\n",
    "\n",
    "$z = w^Tx = \\sum_i w_i x_i = log\\left( \\frac{p(y=1 | x,w)}{1 - p(y=1 | x,w)} \\right)$\n",
    "\n",
    "Transformando a verossimilhança em NLL, pra ficar com cara de função custo\n",
    "\n",
    "$$J(w) = NLL(w) = -logL(w) = \\sum_i^N -y^i log\\left( \\sigma(z^i) \\right) - (1 - y^i)log\\left( 1 - \\sigma(z^i)\\right)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5LOQl5q_sIZ7"
   },
   "source": [
    "### regularization $\\ell^2$ as a prior over parameters\n",
    "\n",
    "Como $$p \\sim Bern (y | x, w)$$ ao se admitir que os erros estão distribuídos de acordo com uma distribuição normal\n",
    "\n",
    "$$ p \\sim Bern(y|x,w)\\mathcal{N}(w|0,\\tau^2) $$\n",
    "\n",
    "ao se construir a NLL, o que vai surgir terá uma cara mais ou menos parecida com\n",
    "\n",
    "$$ J(w) = \\frac{1}{N} \\sum_i^N ( \\cdots )^2 + \\frac{1}{\\tau^2}\\sum_j^D ||w_j||^2  $$\n",
    "\n",
    "então basicamente o parâmetro $\\lambda = \\frac{1}{\\tau^2}$ da regularização, que é associado ao inverso da complexidade do modelo, tem pode também ser interpretado como o grau de dispersão dos parâmetros."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1Z0jwWZMUJFU"
   },
   "source": [
    "### imbalanced classes handling\n",
    "\n",
    "Primeira tentativa default: tree-based algorithm.\n",
    "\n",
    "Eventualmente é preciso recorrer a resampling methods:\n",
    "\n",
    "1. random **oversampling**, samples from minority class (tends to overfitting)\n",
    "2. random **undersamplng**, removes samples for majority (tends to remove useful information -- o que pode ser contornado com uma combinação de undersampling e bagging, mas ai aumenta os falso positivos porque aumenta o bias atraves da propria amostragem)\n",
    "3. synthetic minority oversampling technique, **SMOTE**, basicamente interpola k-nearest neighbours from minority\n",
    "\n",
    "Finalmente, a escolha de uma métrica adequada é importante.\n",
    "\n",
    "[How to Choose Right Metric for Evaluating ML Model @ Kaggle](https://www.kaggle.com/vipulgandhi/how-to-choose-right-metric-for-evaluating-ml-model)\n",
    "\n",
    "1. **precision**, the ability of a classification model to identify only the relevant data points, $$\\Large \\text{precision} = \\frac{\\text{true positive}}{\\text{(true + false) positive}}$$\n",
    "\n",
    "$$\\small \\text{precision} = \\frac{\\text{terroristas corretamente identificados}}{\\text{terroristas corretamente identificados + pessoas incorretamente identificadas como terroristas}}$$\n",
    "\n",
    "2. **recall**,  the ability of a model to find all the relevant cases within a dataset, $$\\Large \\text{recall} = \\frac{\\text{true positive}}{\\text{true positive} + \\text{false negative}}$$\n",
    "\n",
    "$$\\small \\text{recall} = \\frac{\\text{terroristas corretamente identificados}}{\\text{terroristas corretamente identificados + incorretamente classificados como não terroristas}}$$\n",
    "\n",
    "3. **f1 score**, média harmonica entre precision e recall, $$\\Large F_1 = 2 \\times \\frac{\\text{precision} \\times \\text{recall}}{\\text{precision} + \\text{recall}}$$ maximizar $F_1$ garante que o modelo tende a ser balanceado.\n",
    "\n",
    "4. matthews correlation coefficient, **mcc**, também excelente para classes desbalanceadas $$\\text{MCC} = \\frac{\\text{TP}\\cdot\\text{TN} - \\text{FP}\\cdot\\text{FN}}{\\sqrt{ (\\text{TP} + \\text{FP})\\cdot(\\text{TP}+\\text{FN})\\cdot(\\text{TN}+\\text{FP})\\cdot(\\text{TN}+\\text{FN}) }}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RgC9lPSii7DX"
   },
   "source": [
    "### model calibration\n",
    "\n",
    "No caso da regressão logística a minimização da cross-entropy/log-loss produz um modelo razoavelmente bem calibrado, dado que $$\\frac{\\partial J}{\\partial w_j}=0$$ chega em $$\\sum^{N} \\hat{y}(x^i) = \\sum^{N}y^i$$ então a probabilidade do estimador reflete a probabilidade do treino.\n",
    "\n",
    "[Notes on classification probability calibration @ Kaggle](https://www.kaggle.com/residentmario/notes-on-classification-probability-calibration)\n",
    "\n",
    "[How and When to Use a Calibrated Classification Model with scikit-learn](https://machinelearningmastery.com/calibrated-classification-model-in-scikit-learn/)\n",
    "\n",
    "[Probability Calibration Essentials (with code) - Analytics Vidhya - Medium](https://medium.com/analytics-vidhya/probability-calibration-essentials-with-code-6c446db74265)\n",
    "\n",
    "[Probability Calibration for Imbalanced Dataset - Towards Data Science](https://towardsdatascience.com/probability-calibration-for-imbalanced-dataset-64af3730eaab)\n",
    "\n",
    "A great way of checking how a classifier's probability forecasting is performing on your dataset of interest is using a so-called **calibration curve**. The calibration curve works by\n",
    "\n",
    "1. sorting the probabilities assigned to the records being predicted by the probability reported by the classifier\n",
    "2. calculating the `fraction_of_positives` which is the percentage of records in the chosen bin which actually belong to the dominant class\n",
    "3. calculating `mean_predicted_value`, which is the mean probability of these points belonging to the dominant class reported by the algorithm\n",
    "\n",
    "se o `predict_proba` é acurado então o percentual da classe dominante deve ser parecido com a media da probabilidade da classe dominante em cada bin."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VDjxw8gxWOYd"
   },
   "source": [
    "### generalization to multiclass\n",
    "\n",
    "No caso da regressão logística a generalização para classificação multiclasse é natural\n",
    "\n",
    "1. sigmoid to softmax $$p(y=1 | w,x) = \\frac{1}{1+e^{-w^T x}} \\longrightarrow p(y=k | w,x) = \\frac{e_k^{-w^T x}}{\\sum_j^K e_j^{-w^T x}}$$\n",
    "2. bernoulli to multinomial $$f(k,p) = p^k (1-p)^{1-k}, k\\in \\{0,1\\} \\longrightarrow f(x_1,\\dots,x_k,p_1,\\dots,p_k) = \\frac{n!}{x_1!\\dots x_k!}\\prod_i^k p_i^{x_i}$$\n",
    "\n",
    "Mas nem todos os métodos podem ser adaptados dessa forma, daí surgem algumas estratégias, dentre as quais as mais famosas são, dadas $C$ classes:\n",
    "\n",
    "**one-vs-all**, $C$ classificadores binários $h_\\theta^1, \\dots, h_\\theta^C$ são treinados, de tal forma que todos os exemplos das classes $\\neg c$ são negativos, na predição $$max \\; h_\\theta (x)$$ é escolhido. Métodos que podem ser adaptados usando essa abordagem são svms, ensembles e tree-based.\n",
    "\n",
    "**one-vs-one**, $\\frac{C(C-1)}{2}$ classificadores binários são treinados em cada par de classes. O resto é parecido com o OVA.\n",
    "\n",
    "Comparativamente, OvA fita mais rápido e é uma melhor opção pra muitas classes, mas sofre de problemas de desbalanceamentos muito facilmente. OvO pode ser melhor para menos classes, mas computacionalmente é caro.\n",
    "\n",
    "Além disso há outros métodos naturalmente multiclass, como redes neurais e tree-based.\n",
    "\n",
    "[Tips and Tricks for Multi-Class Classification - Mohammed Terry-Jack - Medium](https://medium.com/@b.terryjack/tips-and-tricks-for-multi-class-classification-c184ae1c8ffc)\n",
    "\n",
    "```\n",
    "Tips and Tricks for Multi-Class Classification - Mohammed Terry-Jack @ Medium\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GckpEJpMpWr7"
   },
   "source": [
    "## prediction\n",
    "\n",
    "Dados o conjunto de pesos associados a cada feature em linhas gerais as predições são feitas da seguinte forma\n",
    "\n",
    "1. calcula $w^T x$\n",
    "2. calcula $ log \\left(\\frac{p}{1-p} \\right) = w^T x  \\longrightarrow p = \\frac{1}{1+e^{-w^T x}}$\n",
    "/\n",
    "em um problema de classificação binário, a partir de $p$ é possível associar $0$ ou $1$ à observação."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "l-_AjqYApXcF"
   },
   "source": [
    "## metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8uPD-kepOiYJ"
   },
   "source": [
    "### auc vs log-loss\n",
    "\n",
    "[Understanding binary cross-entropy / log loss: a visual explanation](https://towardsdatascience.com/understanding-binary-cross-entropy-log-loss-a-visual-explanation-a3ac6025181a)\n",
    "\n",
    "AUC é uma métrica de ordenação relativa e portanto é insensível a classificadores descalibrados. \n",
    "\n",
    "Uma solução é usar Precision-Recall\n",
    "\n",
    "Conceitualmente Log-Loss captura a divergência entre a probabilidade do modelo e a classe da observação."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hGw8JhQ7V9yQ"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CcbBV5Z7wjI8"
   },
   "source": [
    "# math of dimensionality reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "M9F8L4lRxuwK"
   },
   "source": [
    "## pca\n",
    "\n",
    "1. normaliza\n",
    "2. autovetor & autovalor da matriz de covariância -- ou correlação ou aplicar uma SVD [[1]](https://medium.com/@jonathan_hui/machine-learning-singular-value-decomposition-svd-principal-component-analysis-pca-1d45e885e491)\n",
    "3. ordena e escolhe os k autovetores com maiores autovalores\n",
    "4. constrói uma matriz W com esses k autovetores\n",
    "5. leva X em Y através de W\n",
    "\n",
    "Contrastando: PCA encontra eixos de máxima variância. LDA cria eixos de máxima separabilidade entre classes. \n",
    "\n",
    "Computacionalmente o LDA k classes (k>2) é executado conforme os seguintes passos\n",
    "\n",
    "1. k centroides são definidos, um para cada classe\n",
    "2. um centroide adicional é posicionado de tal forma a maximizar a quantidade $$\\frac{d_1^2 + \\dots + d_k^2}{s_1^2 + \\dots + s_k^2}$$\n",
    "3. um plano (k-1)-dimensional é definido a partir disso\n",
    "\n",
    "Ou seja, LDA dropa 1 dimensão apenas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "s4oxefg36RGm"
   },
   "source": [
    "```\n",
    "[1] Machine Learning - Singular Value Decomposition (SVD) & Principal Component Analysis (PCA)\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "revisao-e-aprofundamento.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
