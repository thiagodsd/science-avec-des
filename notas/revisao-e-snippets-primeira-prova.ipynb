{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CX3-taG6tmHf"
   },
   "outputs": [],
   "source": [
    "import numpy             as np\n",
    "import pandas            as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7I-pizlmYGDz"
   },
   "source": [
    "# machine learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3deOvX81JG2n"
   },
   "source": [
    "[_a few useful things to know about machine learning_](https://homes.cs.washington.edu/~pedrod/papers/cacm12.pdf) by pedro domingos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "j-0hdzTktNZz"
   },
   "source": [
    "**learning = representation + evaluation + optimization**\n",
    "\n",
    "choosing   a   <u>representation</u>  for  a  learner  is  tantamount  to  choosing  the  set  of  classifiers  that  it  can  possibly  learn, this  set  is  called  the hypothesis   space   of   the   learner\n",
    "\n",
    "an   <u>evaluation   function</u>    -- also    called    objective    function  or scoring  function --  is  needed  to  distinguish   good   classifiers   from   bad   ones\n",
    "\n",
    "a  method  to  search  among  the  classifiers  in  the  language  for  the  highest-scoring   one,   the   choice   of   <u>optimization   technique</u>   is   key   to   the   efficiency   of   the   learner,   and   also   helps   determine   the   classifier   produced  if  the  evaluation  function  has  more  than  one  optimum "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rwmKv_sUtTAg"
   },
   "source": [
    "**it's generalization that counts**\n",
    "\n",
    "set some of the data aside from the beginning, and only use it to test  your  chosen  classifier  at  the  very  end,  followed  by  learning  your  final  classifier on the whole data\n",
    "\n",
    "of  course,  <u/>holding  out</u>  data  reduces  the  amount  available for training, this can be mitigated  by  doing  <u>cross-validation</u>,  randomly dividing your training data into -- say -- 10 subsets, holding out each one while training on the rest, testing each learned  classifier  on  the  examples  it  did  not  see,  and  averaging  the  results  to  see  how  well  the  particular  parameter setting does"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jpHYPmUytS5g"
   },
   "source": [
    "**data alone is not enough**\n",
    "\n",
    "every  learner  must  embody  some  knowledge  or  assumptions beyond the data it is given in  order  to  generalize  beyond  it, this  notion  was  formalized  by  wolpert  in  his famous <u>no free lunch</u> theorems\n",
    "\n",
    "luckily, the functions we  want  to  learn  in  the  real  world  are  not drawn uniformly from the set of all mathematically possible functions, in fact,  very  general  <u>assumptions</u> -- like  smoothness,   similar   examples   having   similar   classes,   limited   dependences,   or   limited   complexity -- are   often enough to do very well, and this is  a  large  part  of  why  machine  learning  has  been  so  successful\n",
    "\n",
    "a corollary of this is that one of the key criteria for choosing a <u>representation</u>  is  which  kinds  of  knowledge  are  easily  expressed  in  it  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jquuS1gStSxq"
   },
   "source": [
    "**overfitting has many faces**\n",
    "\n",
    "\"overfitting = model hallucination\", best definition so far \n",
    "\n",
    "one  way  to  understand  overfitting  is  by  decomposing  generalization error into <u>bias and variance</u>\n",
    "\n",
    "1. bias  is  a  learnerâ€™s  tendency  to  consistently  learn  the  same  wrong  thing\n",
    "2. variance  is  the  tendency  to  learn  random things irrespective of the real signal \n",
    "\n",
    "thus,  contrary to intuition, a more powerful learner is not necessarily better than a less powerful one\n",
    "\n",
    "strong  false  assumptions can  be  better  than  weak  true  ones,  because  a  learner  with  the  latter  needs  more  data to avoid overfitting -- por exemplo, em dados gerados via if-else\n",
    "\n",
    "1. naive bayes, hipotese de classes linearmente separaveis falsa\n",
    "2. decision trees, hipotese de pre-condicoes para as classes verdadeira \n",
    "\n",
    "naive bayes melhor para amostras pequenas\n",
    "\n",
    "besides      cross-validation,      there      are  many  methods  to  combat  overfitting,  the  most  popular  one  is  adding  a <u>regularization</u> term to the evaluation function -- for example applying penalization to classifiers with more structure\n",
    "\n",
    "another  option  is to perform a <u>statistical significance test</u> like chi-square before adding new structure\n",
    "\n",
    "the problem of multiple testing  is  closely related to overfitting ... a better approach is to con-trol   the   fraction   of   falsely   accepted   non-null   hypotheses,   known   as   the   <u>false discovery rate</u>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "td_dShf9tSpa"
   },
   "source": [
    "**intuition fails in high dimensions**\n",
    "\n",
    "after  overfitting,  the  biggest  problem  in  machine  learning  is  the  <u>curse   of   dimensionality</u>\n",
    "\n",
    "one  might  think  that  gathering  more  features  never  hurts,  since  at  worst  they  provide no new information about the class,  but  in  fact  their  benefits  may  be outweighed by the curse of dimensionality\n",
    "\n",
    "fortunately,  there  is  an  effect  that  partly   counteracts   the   curse,   which   might  be  called  the  <u>blessing  of  non-uniformity</u>,    in    most    applications    examples   are   not   spread   uniformly   throughout   the   instance   space,   but   are  concentrated  on  or  near  a  lower-dimensional  manifold  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TA1jfcewtShQ"
   },
   "source": [
    "**theoretical guarantees are not what they seem**\n",
    "\n",
    "another  common  type  of  theoretical  guarantee  is  asymptoticm  given  infinite  data,  the  learner  is  guaranteed  to  output  the  correct  classifier\n",
    "\n",
    "in practice, we are seldom in the asymptotic  regime  -- also  known  as  <u>asymptopia</u> --,  and,  because  of  the  bias-variance  trade-off  i  discussed  earlier,  if  learner a is better than learner b given infinite  data,  b  is  often  better  than  a  given finite data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "L6dEsnF4tSUo"
   },
   "source": [
    "**feature engineering is the key**\n",
    "\n",
    "at  the  end  of  the  day,  some  machine  learning  projects  succeed  and  some  fail ... easily  the  most  important  factor  is  the  features  used\n",
    "\n",
    "this -- feature engineering -- is typically where most of the effort in a machine learning project goes, it is often also one of the most interesting parts,  where  intuition,  creativity  and  <u>black  art</u>  are  as  important  as  the  technical stuff\n",
    "\n",
    "machine  learning  is  not  a  one-shot process of building a dataset and running a learner, but rather an iterative  process  of  running  the  learner,  analyzing  the  results,  modifying  the  data  and-or  the  learner,  and  repeating"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xYHg3vzCtSNL"
   },
   "source": [
    "**more data beats a clever algorithm**\n",
    "\n",
    "as  a  rule  of thumb, a dumb algorithm with lots and lots of data beats a clever one with modest  amounts  of  it\n",
    "\n",
    "this  does  bring  up  another  problem,  however:  scalability\n",
    "\n",
    "in  most  of  computer  science,  the  two  main  limited  resources  are  time  and  memory,  in  machine  learning,  there  is  a  third  one,  training  data\n",
    "\n",
    "this  leads  to  a  <u>paradox</u>,  even  though  in  principle more data means that more complex classifiers can be learned, in practice  simpler  classifiers  wind  up  being   used,   because   complex   ones   take  too  long  to  learn\n",
    "\n",
    "with  nonuniformly  distributed  data,  learners  can  produce  widely  different  frontiers  while  still  making  the  same  predictions in the regions that matter ...  this  also  helps  explain  why  powerful learners can be unstable but still  accurate\n",
    "\n",
    "as a rule, it pays to <u>try the simplest learners first</u>\n",
    "\n",
    "1. naive bayes before   logistic   regression\n",
    "2. k-nearest neighbor  before  support  vector  machines\n",
    "\n",
    "learners  can  be  divided  into  two  major  types\n",
    "\n",
    "1. <u>parametric</u>, those  whose  representation has a fixed size, like linear classifiers\n",
    "2. <u>nonparametric</u>, those whose representation can  grow  with  the  data,  like  decision  trees\n",
    "\n",
    "the latter are sometimes called nonparametric   learners,   but   this   is   somewhat    unfortunate,    since    they    usually  wind  up  learning  many  more  parameters   than   parametric   ones\n",
    "\n",
    "fixed-size  learners  can  only  take  advantage  of  so  much  data -- notice  how  the accuracy of naive bayes asymptotes at  around  70%\n",
    "\n",
    "variable-size learners can in principle learn any function  given  sufficient  data,  but  in  practice they may not, because of limitations of the algorithm  -- for example, greedy  search  falls  into  local  optima --  or  computational  cost, also,  because  of  the  curse  of  dimensionality,  no  existing amount of data may be enough\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iFimxO-LtSGN"
   },
   "source": [
    "**learn many models, not just one**\n",
    "\n",
    "historica e cronologicamente\n",
    "\n",
    "1. everyone  had  a  favorite  learner\n",
    "2. most  effort  went  into  trying  many  variations  of  it  and  selecting  the  best  one\n",
    "3.  empirical     comparisons     showed  that  the  best  learner  varies  from  application  to  application\n",
    "4. systems   containing   many   different   learners  started  to  appear\n",
    "5. effort  now  went  into  trying  many  variations  of  many  learners,  and  still  selecting  just  the   best   one\n",
    "6.  researchers   noticed  that,  if  instead  of  selecting  the  best  variation  found,  we  combine  many  variations,  the  results  are  better\n",
    "\n",
    "<u>model ensembles</u> is now standard\n",
    "\n",
    "<u>bagging</u>,  simply  generate  random  variations  of  the  training set by resampling, learn a classifier on  each,  and  combine  the  results  by  voting -- reduces variance, slightly increases bias\n",
    "\n",
    "<u>boosting</u>, training  examples  have  weights,  and  these  are  varied  so  that  each  new  classifier  focuses  on  the  examples  the  previous  ones tended to get wrong\n",
    "\n",
    "<u>stacking</u>, the   outputs   of   individual   classifiers   become  the  inputs  of  a  higher-level  learner  that  figures  out  how  best  to  combine them"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yphokr2NtR-s"
   },
   "source": [
    "**simplicity does not imply accuracy**\n",
    "\n",
    "the   conclusion   is   that   simpler   hypotheses should be preferred because simplicity  is  a  virtue  in  its  own  right,  not because of a hypothetical connection  with  accuracy,  this  is  probably  what <u>occam</u> meant in the first place"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KLT5UG4LtR2w"
   },
   "source": [
    "**representable does not imply learnable**\n",
    "\n",
    "all  representations  used  in  variable-size  learners  have  associated theorems  of  the  form  every  function  can  be  represented,  or  approximated  arbitrarily   closely,   using   this   representation \n",
    "\n",
    "however, just   because  a  function  can  be  represented  does  not  mean  it  can  be  learned"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pN18_HWXuego"
   },
   "source": [
    "**correlation does not imply causation**\n",
    "\n",
    "sempre bom lembrar que filosoficamente causalidade e um negocio um pouco mais complicado do que parece [[1]](https://daveduncombe.wordpress.com/2015/01/02/hume-on-causation/), [[2]](https://spaceandmotion.com/Philosophy-David-Hume-Philosopher.htm)\n",
    "\n",
    "    [1] Hume on Causation @ David Duncombe\n",
    "    [2] David Hume Philosophy: Explaining Hume's Problem of Causation, Skepticism. Philosopher David Hume Quotes\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mHemfsKVf3SF"
   },
   "source": [
    "## 1 classical learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UVp81NkdUw0X"
   },
   "source": [
    "paradigmas que eclipsam supervised vs unsupervised"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9lgNOkECUyiw"
   },
   "source": [
    "\n",
    "**batch vs online**\n",
    "\n",
    "+ _batch learning_ e o aprendizado feito de uma vez com todo o conjunto de dados disponivel, de modo que se algo precisa ser atualizado -- um novo tipo de spam -- e necessario treinar tudo novamente, novo spam jutamente com todos os antigos treinados anteriormente\n",
    "+ _online learning_ e o aprendizado incrimental, observacao a observacao ou atraves de mini-batches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hBfjAOojOh3B"
   },
   "source": [
    "**instance-based vs model-based**\n",
    "\n",
    "+ _instance-based_ generaliza novos casos a partir dos casos aprendidos, atraves de alguma metrica de similaridade\n",
    "+ _model-based_ generaliza novos casos atraves dos parametros de um modelo construido a priori"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wGmvCub7Oqdy"
   },
   "source": [
    "**generative vs discriminative**\n",
    "\n",
    "+ _generative_, alguma forma funcional e atribuida as $p(y)$ e $p(x|y)$ e a partir disso parametros das distribuicoes sao aprendidos, recorrentemente usando-se bayes para estimar $p(y|x)$ \n",
    "+ _discriminative_, alguma forma funciona e atribuida a $p(y|x)$ e a partir disso parametros sao aprendidos\n",
    "\n",
    "no contexto de classificacao _generative_ aprende as distribuicoes das classes e _discriminative_ aprende fronteiras entre classes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ldEt8BQoTcwc"
   },
   "source": [
    "**parametric vs nonparametric**\n",
    "\n",
    "+ _parametric_, uma forma funcional governada por um numero fixo de parametros e escolhida \n",
    "+ _nonparametric_, complexidade do modelo virtualmente aumenta com o aumento de observacoes e features\n",
    "\n",
    "metodos parametricos em geral sao computacionalmente mais razoaveis entretanto a qualidade dos modelos eventualmente esta associada a boa escolha da forma funcional, nesse sentido a abordagem nao-parametrica resolve parcialmente a inconveniencia ao estabelecer menos hipoteses sobre os dados\n",
    "\n",
    "parcialmente porque em metodos nao-parametricos eventualmente a qualidade do modelo volta a depender da boa escolha de alguma coisa, como por exemplo em um problema de kernel density estimation \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "t6OACp5TBRlM"
   },
   "source": [
    "**adaptive basis function**\n",
    "\n",
    "de forma simplificada \n",
    "\n",
    "1. ha o conjunto de linear methods, capazes de realizar tarefas de regressao e classificacao\n",
    "2. ha o conjunto de kernel methods, que ampliam os dominios de atuacao dos linear methods\n",
    "\n",
    "o problema intrinseco aos kernel methods e a escolha do kernel adequado para o problema\n",
    "\n",
    "teoricamente isso pode ser contornado via kernel learning, mas quase sempre isso e computacionalmente caro -- e possivel ainda recorrer a combinacao de kernels, mas isso nao muda o fato de que bons kernels precisam ser escolhidos\n",
    "\n",
    "nesse cenario os adaptive basis function methods surgem como alternativa, em que o aprendizado se da diretamente no espaco de features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Pg8EyLNzgPwk"
   },
   "source": [
    "### 1.1 supervised"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KBKidjQfklvT"
   },
   "source": [
    "#### 1.1.1 regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-AQC9nqeIoUL"
   },
   "source": [
    "##### 1.1.1.1 linear regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ulh-yrL1I204"
   },
   "source": [
    "detalhes [[1]](https://www.kaggle.com/kashnitsky/topic-4-linear-models-part-1-ols)\n",
    "\n",
    "```\n",
    "[1] Topic 4. Linear models. Part 1. OLS @ Kaggle\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cftq3yy6I8WI"
   },
   "source": [
    "##### 1.1.1.2 polynomial regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Di3bbRMXJG1Z"
   },
   "source": [
    "detalhes [[1]](https://github.com/thiagodsd/science-avec-des/blob/master/notebooks_annotations/regression.ipynb)\n",
    "\n",
    "```\n",
    "[1] science-avec-des/regression.ipynb @ github\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "upMqNOrjJNMK"
   },
   "source": [
    "##### 1.1.1.3 regularized regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tZFOgshiC2xX"
   },
   "source": [
    "detalhes [[1]](https://www.kaggle.com/kashnitsky/topic-4-linear-models-part-1-ols), [[2]](https://github.com/thiagodsd/science-avec-des/blob/master/notebooks_annotations/regression.ipynb)\n",
    "\n",
    "tikhonov regularization -- da ridge regression, solucao exata top $$\\mathcal{L}\\left(\\textbf{X}, \\textbf{y}, \\textbf{w} \\right) = \\frac{1}{2n} \\left\\| \\textbf{y} - \\textbf{X} \\textbf{w} \\right\\|_2^2 + \\left\\| \\Gamma \\textbf{w}\\right\\|^2 \\longrightarrow \\textbf{w} = \\left(\\textbf{X}^{\\text{T}} \\textbf{X} + \\lambda \\textbf{E}\\right)^{-1} \\textbf{X}^{\\text{T}} \\textbf{y}$$\n",
    "\n",
    "```\n",
    "[1] Topic 4. Linear models. Part 1. OLS @ Kaggle\n",
    "[2] science-avec-des/regression.ipynb @ github\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8gBEzVxWHtNF"
   },
   "source": [
    "###### snippets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3UbdrF-THvB9"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model    import Ridge\n",
    "from sklearn.metrics         import mean_absolute_error\n",
    "\n",
    "ridge_cv   = GridSearchCV(Ridge(fit_intercept=True), {'alpha': [2.37**i for i in range(-8, 8)]}, scoring='neg_mean_absolute_error', cv=5)\n",
    "ridge_cv.fit(X, y)\n",
    "print(ridge_cv.best_params_['alpha'])\n",
    "\n",
    "ridreg = Ridge( alpha=ridge_cv.best_params_['alpha'], fit_intercept=True )\n",
    "ridreg.fit(X, y)\n",
    "\n",
    "display( mean_absolute_error( y, ridreg.predict(X) ) )\n",
    "display( pd.Series(ridreg.coef_.flatten(), X.columns.values.flatten()).sort_values().plot(kind='bar') )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "07PTAU6Pknrd"
   },
   "source": [
    "#### 1.1.2 classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GKe6a-U8w-Lz"
   },
   "source": [
    "##### 1.1.2.1 logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8niBL0xHIOal"
   },
   "source": [
    "sempre bom lembrar que probabilisticamente quase sempre ha uma $p(y|x,w)$ que da origem a variavel resposta e dai\n",
    "\n",
    "$p \\sim \\mathcal{N}(y|w^T \\phi(x), \\sigma^2)$, regressao\n",
    "\n",
    "$p \\sim Bern\\left( y | \\frac{1}{1+e^{-w^T x}} \\right)$, regressao logistica\n",
    "\n",
    "tricky, mas $p(n) \\sim Bern$\n",
    "\n",
    " 1. $p \\, \\text{para} \\, n=1$\n",
    " 2. $1-p \\, \\text{para} \\, n=0$\n",
    " \n",
    "entao para o caso binario por exemplo\n",
    "\n",
    "1. $p_+ = \\sigma(w^T x)$\n",
    "2. $p_- = 1-\\sigma(w^T x) = \\sigma(-w^T x)$ \n",
    "\n",
    "**odds** $$\\frac{\\text{event}}{\\text{not event}} = \\frac{p}{1-p} \\in [0, \\infty)$$\n",
    "\n",
    "**log odds** $$log \\left(\\frac{p}{1-p} \\right) \\in \\mathbb{R}$$\n",
    "\n",
    "**prediction**\n",
    "\n",
    "dados os pesos associados a cada feature\n",
    "\n",
    "1. calcula $w^T x$\n",
    "2. calcula $ log \\left(\\frac{p}{1-p} \\right) = w^T x  \\longrightarrow p = \\frac{1}{1+e^{-w^T x}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JNvWF9IhlZDM"
   },
   "source": [
    "**objective** $$ J(X,y,w) = \\sum_i^\\ell log(1 + e^{-y_i w^T x_i}) + \\frac{1}{C}||w||^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vE5DLZ1_jeTA"
   },
   "source": [
    "###### snippets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wzf2Gs8UkK2v"
   },
   "source": [
    "`sklearn.linear_model.LogisticRegression`\n",
    "\n",
    "1. `penalty, default='l2'`\n",
    "2. `C, default=1.0`, inverse of regularization strength, must be a positive float, like in support vector machines, smaller values specify stronger regularization\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_wyKcxpKj9E6"
   },
   "source": [
    "polynomial logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JTROt8-XjgEJ"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%config     InlineBackend.figure_format = 'retina'\n",
    "\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LogisticRegression, LogisticRegressionCV\n",
    "from sklearn.model_selection import cross_val_score, StratifiedKFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "poly = PolynomialFeatures(degree=7)\n",
    "X_poly = poly.fit_transform(X)\n",
    "\n",
    "C = 1e-2\n",
    "logit = LogisticRegression(C=C, random_state=17)\n",
    "logit.fit(X_poly, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nX6IV0FcnPcK"
   },
   "source": [
    "grid search cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zByeE-eEnLaA"
   },
   "outputs": [],
   "source": [
    "skf            = StratifiedKFold(n_splits=5, shuffle=True, random_state=17)\n",
    "c_values       = np.logspace(-2, 3, 500)\n",
    "logit_searcher = LogisticRegressionCV(Cs=c_values, cv=skf, verbose=1, n_jobs=-1)\n",
    "\n",
    "logit_searcher.fit(X_poly, y)\n",
    "\n",
    "logit_searcher.C_\n",
    "plt.plot(c_values, np.mean(logit_searcher.scores_[1], axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZD8dA1HyxKpY"
   },
   "source": [
    "##### 1.1.2.2 decision trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8FEOUkkgLxN_"
   },
   "source": [
    "decision tree via CART pode ser entendido como um particionamento recursivo do espaco de input $$f(x) = \\left< y | x \\right> = \\sum_m w_m \\phi(x, v_m)$$ em que $v_m$ codifica o algoritmo de escolha e esta associado a m-esima regiao"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UDJ8gBrXzuI3"
   },
   "source": [
    "* **shannon's entropy** $$S = - \\sum_i^N p_i log p_i $$ higher entropy higher disorder\n",
    "\n",
    "<br/>\n",
    "\n",
    "* **information gain** over variable Q, same as entropy reduction $$IG(Q) = S_0 - \\sum_i^q \\frac{N_i}{N} S_i$$ onde $S_0$ e a entopia inicial sem splits, e a soma seguinte leva em conta os $q$ splits cada um contendo $N_i$ elementos\n",
    "\n",
    "* **gini impurity** $$G = 1 - \\sum_ip_i^2$$ que quando maximizado equivale maximizar o numero de de pares de objetos de mesma classe na mesma sub-arvore\n",
    "\n",
    "<br/>\n",
    "\n",
    "algoritmos como CART minimizam entropia ou maximizam gini impurity, em **decision tree regressor** o criterio de splitting passa a ser a variancia $$\\text{var} = \\frac{1}{\\ell} \\sum_i^\\ell  \\left( y_i -  \\frac{1}{\\ell}\\sum_j^\\ell y_j \\right)^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UeaqG--81Pyd"
   },
   "source": [
    "###### snippets\n",
    "\n",
    "+ `max_depth` the maximum depth of the tree\n",
    "+ `max_features`, the maximum number of features with which to search for the best partition -- this is necessary with a large number of features because it would be \"expensive\" to search for partitions for all features\n",
    "+ `min_samples_leaf`, the minimum number of samples in a leaf, this parameter prevents creating trees where any leaf would have only a few members\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cI7x4m_bVKMg"
   },
   "source": [
    "classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "m9y17YBe1SHq"
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "clf_tree = DecisionTreeClassifier(criterion='entropy', max_depth=3, random_state=17)\n",
    "clf_tree.fit(train_data, train_labels)\n",
    "pred = clf_tree.predict(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "F091gC34GTgB"
   },
   "source": [
    "regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xcw41FQJ2hfM"
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "reg_tree = DecisionTreeRegressor(max_depth=5, random_state=17)\n",
    "\n",
    "reg_tree.fit(X_train, y_train)\n",
    "reg_tree_pred = reg_tree.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "P-AY8hHYoNnT"
   },
   "source": [
    "##### 1.1.2.3 svm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "l_AZtldbCQjo"
   },
   "source": [
    "o famigerado kernel trick [ref](https://sebastianraschka.com/Articles/2014_kernel_pca.html#kernel-functions-and-the-kernel-trick)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6XPPa_C3pHFe"
   },
   "source": [
    "###### snippets\n",
    "\n",
    "importantissimo: a `cross_val_score` retorna o score no **teste** sempre, entao algo como\n",
    "\n",
    "```python\n",
    "scr_f1 = cross_val_score(SVC(kernel='linear', C=1), df.drop(labels=['target'], axis=1), df['target'], cv=10, scoring='f1')\n",
    "\n",
    "display(scr_f1)\n",
    "display(np.mean(scr_f1))\n",
    "```\n",
    "\n",
    "vai dar problema se uma questao pede score no **treino**, um jeito mais geral de fazer o cross-validation esta escrito abaixo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cWktHHe-oPXE"
   },
   "outputs": [],
   "source": [
    "from sklearn.svm             import SVC\n",
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "from sklearn.metrics         import f1_score\n",
    "\n",
    "df = pd.read_csv('{}/itub/Questoes/classificacao_1.csv'.format(PATH))\n",
    "df.head()\n",
    "\n",
    "scores = list()\n",
    "\n",
    "X = df.drop(labels=['target'], inplace=False, axis=1)\n",
    "y = df['target']\n",
    "\n",
    "for train_id, test_id in KFold(n_splits=10).split(df):\n",
    "  X_train, X_test = X.iloc[train_id], X.iloc[test_id]\n",
    "  y_train, y_test = y.iloc[train_id], y.iloc[test_id]\n",
    "\n",
    "  svc = SVC(kernel='linear', C=1)\n",
    "  svc.fit(X_train, y_train)\n",
    "  \n",
    "  # NAS PARTICOES DE TREINO !\n",
    "  # y_pred = svc.predict(X_test)\n",
    "  # scores.append(f1_score(y_test, y_pred))\n",
    "\n",
    "  y_pred = svc.predict(X_train)\n",
    "  scores.append(f1_score(y_train, y_pred))\n",
    "\n",
    "display(scores)\n",
    "display(np.mean(scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ol9C4y2uxRaJ"
   },
   "source": [
    "##### 1.1.2.4 knn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6inhK_-TS9mj"
   },
   "source": [
    "para classificao sob a condicao de $\\frac{k}{N} \\to 0$ knn tende a um classificador bayesiano otimo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tzku4GzQIhWK"
   },
   "source": [
    "do ponto de vista teorico existe uma conexao muito profunda entre knn e kernel methods, em linhas gerais dada a regiao $\\mathcal{R}$ do espaco de dados $\\mathcal{D}$ a probabilidade de $x$ ser observado em $\\mathcal{R}$ e $$P = \\int_{\\mathcal{R}} p(x)dx$$ entao se $|\\mathcal{D}|=N$, entao de $N$ observacoes $K$ caem em $\\mathcal{R}$ com probabilidade $$Bin(K|N,p)=\\frac{N!}{K!(N-K)!}p^K (1-p)^{1-K}$$ com $\\left<\\frac{K}{N}\\right>=p$ dos pontos em $\\mathcal{R}$ e $var\\left( \\frac{K}{N} \\right) = \\frac{p(1-p)}{N}$\n",
    "\n",
    "quando $$N \\to \\infty$$ pela binomial $K \\to Np$ e quando $$\\mathcal{R} < \\epsilon$$ pode-se aproximar a distribuicao $p(x) \\approx p, \\text{(constante)}$ e ai $$P = \\int_{\\mathcal{R}} p(x)dx \\approx pV = \\frac{K}{N}V$$ entao finalmente $$p(x) \\approx \\frac{K}{V N}$$\n",
    "\n",
    "mas as condicoes de n grande e volume pequeno sao contraditorias, entao dependendo da hipotese escolhida a equacao anterior da origem a tecnicas diferentes\n",
    "\n",
    "1. **volume constante** implica em determinar k a partir dos dados, dando origem aos **kernel methods**, em que a regiao $\\mathcal{R}$ e aproximada por hipercubos centrados nas observacoes <u>para a partir disso determinar $p(x)$ </u>\n",
    "2. **k constante** implica em determinar o volume a partir dos dados, dando origem ao **knn** <u>para a partir disso determinar $p(x)$ </u>  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ITZSGLsgkUxl"
   },
   "source": [
    "uma forma de partir da equacao anterior e chegar no knn classifier e usando o teorema de bayes\n",
    "\n",
    "se $\\mathcal{R}$ e dividido em $k$ partes, entao $\\sum_k N_k = N$ e $$p(x) = \\frac{K}{NV} \\to p(x|C_k) = \\frac{K_k}{N_k V}$$ e uma forma natural de definir a probabilidade de uma observacao pertecencer a classe $C_k$ e $p(C_k) = \\frac{N_k}{N}$ de modo que o classificador pode ser escrito como $$p(C_k | x) = \\frac{p(x|C_k)p(C_k)}{p(x)} = \\frac{K_k}{N_k V}\\frac{N_k}{N}\\frac{1}{p(x)} = \\frac{K_k}{K}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CGFfvn228X9g"
   },
   "source": [
    "###### snippets\n",
    "\n",
    "+ `weights`, uniform (all weights are equal), distance (the weight is inversely proportional to the distance from the test sample), or any other user-defined function\n",
    "+ `algorithm` (optional), brute, ball_tree, kd_tree, or auto. in the first case, the nearest neighbors for each test case are computed by a grid search over the training set. in the second and third cases, the distances between the examples are stored in a tree to accelerate finding nearest neighbors. if you set this parameter to auto, the right way to find the neighbors will be automatically chosen based on the training set\n",
    "+ `leaf_size` (optional), threshold for switching to grid search if the algorithm for finding neighbors is balltree or kdtree\n",
    "+ `metric`, minkowski, manhattan, euclidean, chebyshev, or other\n",
    "\n",
    "sempre importante normalizar tudo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zDnt_mFf8bYu"
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors       import KNeighborsClassifier\n",
    "from sklearn.preprocessing   import StandardScaler\n",
    "\n",
    "knn    = KNeighborsClassifier(n_neighbors=10)\n",
    "scaler = StandardScaler()\n",
    "\n",
    "X_train_scaled   = scaler.fit_transform(X_train)\n",
    "X_holdout_scaled = scaler.transform(X_holdout)\n",
    "\n",
    "knn.fit(X_train_scaled, y_train)\n",
    "knn_pred = knn.predict(X_holdout_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0Ia2kAxoFej6"
   },
   "source": [
    "alternativamente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8HzIDPQdFS1k"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV, cross_val_score\n",
    "from sklearn.pipeline        import Pipeline\n",
    "from sklearn.neighbors       import KNeighborsClassifier\n",
    "from sklearn.preprocessing   import StandardScaler\n",
    "\n",
    "knn_pipe   = Pipeline([('scaler', StandardScaler()), ('knn', KNeighborsClassifier(n_jobs=-1))])\n",
    "knn_params = {'knn__n_neighbors': range(1, 10)}\n",
    "knn_grid   = GridSearchCV(knn_pipe, knn_params, cv=5, n_jobs=-1, verbose=True)\n",
    "knn_grid.fit(X_train, y_train)\n",
    "\n",
    "print(knn_grid.best_params_, knn_grid.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gaTPBPUhvo7D"
   },
   "source": [
    "##### 1.1.2.5 naive bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "V0SGz0nmCobf"
   },
   "source": [
    "+ particularmente forte em pequenos datasets\n",
    "+ comum em text classification e predicao de doencas\n",
    "+ robusto frente a overfitting\n",
    "\n",
    "$$\\text{naive} \\longleftrightarrow \\text{features mutually independent}$$ \n",
    "\n",
    "e conhecido que a **hipotese de independencia mutua de features** implicita em  $$p(y=c|\\mathcal{D})  \\propto p(y=c|\\mathcal{D})\\prod_j^D p(x_j | y=c, \\mathcal{D})$$ e que por um lado e quase impossivel no mundo real, por outro pode ser violada com alguma tranquilidade, entretanto a menos conhecida **hipotese de classes linearmente separaveis** [[1]](https://www.cs.cornell.edu/courses/cs4780/2018sp/lectures/lecturenote05.html) devido ao maximum likelihood pode tornar modelos naive bayes pessima escolhas, mas valem observacoes\n",
    "\n",
    "1. uma forma de dibrar isso e usar o famigerado kernel trick\n",
    "2. naive bayes **nao e linear em geral**, apenas sob a frequente condicao de que as distribuicoes pertencem a familias exponenciais [ref](https://stats.stackexchange.com/a/142258), alem disso algumas demonstracoes de linearidade mostram linearidade sobre pesos assocaidos as probabilidades e nao necessariamente sobre as features [ref](https://svivek.com/teaching/machine-learning/lectures/slides/naive-bayes/naive-bayes-linear.pdf), entao novamente, e possivel dibrar essa fraqueza com simples aplicacoes de basis functions\n",
    "\n",
    "```\n",
    "[1] Lecture 5: Bayes Classifier and Naive Bayes\n",
    "[ref](https://sebastianraschka.com/Articles/2014_naive_bayes_1.html)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "smrusnnOewnC"
   },
   "source": [
    "###### snippets\n",
    "\n",
    "$p(x|\\text{stuff})$ da equacao anterior pode ter muitas caras, dependendo da natureza das features\n",
    "\n",
    "+ para features binarias, a bernoulli $$p(x) = \\theta^x (1-\\theta)^{1-x}$$\n",
    "+ para features discretas, em geral representando mais de duas categorias, a multinomial $$p(x) = \\frac{N!}{\\prod_j^n x_j!}\\prod_j^n \\theta_j^{x_j}$$\n",
    "+ para features continuas, a gaussiana $$p(x) = \\frac{1}{\\sqrt{2\\pi \\sigma^2}}exp \\left( -\\frac{(x - \\theta)^2}{2\\sigma^2} \\right)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "G_Sk8lvmeyoK"
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB    # continuous features\n",
    "\n",
    "model = GaussianNB()\n",
    "model.fit(X, y)\n",
    "ynew = model.predict(Xnew)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bgKJnOfYft0Q"
   },
   "source": [
    "exemplo de text classification e matriz de confusao"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OflrMXvxfcKy"
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes             import MultinomialNB # discrete features\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline                import make_pipeline\n",
    "from sklearn.metrics                 import confusion_matrix\n",
    "\n",
    "model = make_pipeline(TfidfVectorizer(), MultinomialNB())\n",
    "model.fit(train.data, train.target)\n",
    "labels = model.predict(test.data)\n",
    "\n",
    "#\n",
    "\n",
    "mat = confusion_matrix(test.target, labels)\n",
    "sns.heatmap(mat.T, square = True, annot  = True, fmt ='d', cbar=False,\n",
    "            xticklabels = train.target_names, \n",
    "            yticklabels = train.target_names)\n",
    "plt.xlabel('true label')\n",
    "plt.ylabel('predicted label')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GqwqFsb1gP2g"
   },
   "source": [
    "### 1.2 unsupervised"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "B8reHXlegP7c"
   },
   "source": [
    "#### 1.2.1 Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8m681AMWYxto"
   },
   "source": [
    "##### 1.2.1.1 Hierarchical Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XMlAhVzuY2zS"
   },
   "source": [
    "Strategies for hierarchical clustering generally fall into two types:\n",
    "\n",
    "**Agglomerative**, a \"bottom-up\" approach: each observation starts in its own cluster, and pairs of clusters are merged as one moves up the hierarchy. **Divisive**, a \"top-down\" approach: all observations start in one cluster, and splits are performed recursively as one moves down the hierarchy [ref](https://www.kaggle.com/vipulgandhi/hierarchical-clustering-explanation).\n",
    "\n",
    "Agglomerative clustering possuem algumas formas diferentes de medir dissimilaridade entre grupos, e isso define a variante do metodo.<br/>\n",
    "**single link** onde a distancia minima entre os membros de cada par de grupos e a medida similaridade; capaz de separar coisas nao-elipticas, mas fragil com respeito a ruido entre clusters.<br/>\n",
    "**complete link** em que a distancia maxima entre os membros e a medida de dissimilaridade; lida bem com ruido, mas ha um vies globular nessa abordagem, alem da tendencia de quebrar grandes clusters.<br/>\n",
    "**average link** em que a distancias entre os centroides e a medida de dissimilaridade; lida bem com ruido, mas tambem possui vies globular [ref](https://towardsdatascience.com/understanding-the-concept-of-hierarchical-clustering-technique-c6e8243758ec).\n",
    "\n",
    "Do Divisive clustering surge o **bisecting K-Means**, em que sucessivos 2-Means sao aplicados em cada cluster maior.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kYWEUy8pmS5-"
   },
   "source": [
    "##### 1.2.1.2 K-means\n",
    "\n",
    "[ref](https://www.kaggle.com/vipulgandhi/kmeans-detailed-explanation) com detalhes.\n",
    "\n",
    "* K-Medoids\n",
    "\n",
    "Em geral K-means usa a squared euclidian distance -- provavelmente para evitar o calculo da raiz quadrada. Isso torna o K-means menos robusto a outliers. A proposta do K-Medoids e corrigir isso. A ideia central e encontrar objetos representativos no conjunto de dados [ref](https://www.youtube.com/watch?v=GApaAnGx3Fw). Uma vez definidos os objetos representativos -- medoids -- a atualizacao acontece pela verificacao se algum outro objeto diminui a soma de medidas de similaridade. E mais caro que o K-Means, porem mais robusto.\n",
    "\n",
    "* K-Means++\n",
    "\n",
    "Inicializacoes ruins geram agrupamentos ruins no K-means -- alem de eventualmente aumentar o tempo necessario para convergencia --, entao o K-Means++ tenta corrigir isso, gerando inicializacoes quasi-otimas. A ideia consiste em escolher o primeiro centroide aleatoriamente entre os pontos a serem agrupados e a partir do segundo centroide o posicionamento e feito aleatoriamente de acordo com a probabilidade proporcional ao quadrado da distancia dos centroides existentes. Isso garante que **os centroides sejam inicializados de modo mais distante entre si**. [ref](https://medium.com/machine-learning-algorithms-from-scratch/k-means-clustering-from-scratch-in-python-1675d38eee42)\n",
    "\n",
    "Com respeito a outliers, qualquer metodo de inicializacao e otimizacao vai ser sensivel a outliers dado que a funcao objetivo e a soma de quadrados.\n",
    "\n",
    "* Mini Batch K-Means\n",
    "\n",
    "Mais rapido.\n",
    "\n",
    "* Streaming K-Means\n",
    "\n",
    "Versao online do K-means -- e portanto analoga ao Mini Batch K-Means -- em que novos dados vao surgindo e a partir dos quais a posicao dos clusteres vai sendo atualizadas. Eventualmente e possivel usar hiper-parametros para diminuir a importancia de dados muito antigos alem de ser possivel remover clusteres nao atualizados ha muito tempo. [ref](https://stats.stackexchange.com/questions/222235/explain-streaming-k-means)\n",
    "\n",
    "* K-means vs K-medians\n",
    "\n",
    "Usa mediana no lugar da raiz da distancia euclidiana, entao troca uma metrica $\\ell^2$ por uma $\\ell$.\n",
    "\n",
    "* Elbow vs Silhoutte\n",
    "\n",
    "**Elbow**: Inertia $\\times$ k-esimo cluster.<br/> \n",
    "**Silhouette**: The silhouette value measures how similar a point is to its own cluster (cohesion) compared to other clusters (separation) [ref](https://medium.com/analytics-vidhya/how-to-determine-the-optimal-k-for-k-means-708505d204eb). \n",
    "\n",
    "![](https://wikimedia.org/api/rest_v1/media/math/render/svg/3d80ab22fb291b347b2d9dc3cc7cd614f6b15479)\n",
    "\n",
    "onde $a(i)$ e a media entre i e todos os pontos, representando quao bem o i-esimo ponto esta associado **ao seu cluster**; $b(i)$ e a menor distancia entre e todos os outros pontos **dos outros clusters**, representando a dissimilaridade.\n",
    "\n",
    "Em particular para o caso de GMM a BIC pode ser uma metrica mais adequada para escolha do numero otimo de clusters.\n",
    "\n",
    "* Inertia vs Distortion\n",
    "\n",
    "**Inertia** e a soma das distancias quadraticas entre as amostras o cluster mais proximo. **Distortion** e a media das distancias quadraticas dos centroides aos elementos associados a eles.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "824TlaVHQ-LN"
   },
   "source": [
    "###### snippet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mA48BUl8mVHC"
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans, MiniBatchKMeans\n",
    "\n",
    "agr      = pd.read_csv('{}/itub/Questoes/agrupamento.csv'.format(PATH))\n",
    "centroid = pd.read_csv('{}/itub/Questoes/centroides_iniciais.csv'.format(PATH))\n",
    "\n",
    "display(agr.head())\n",
    "display(agr.shape)\n",
    "display(centroid.head())\n",
    "\n",
    "# elbow method setting cluster centroids\n",
    "\n",
    "inertia = list()\n",
    "for k in range(1, 16):\n",
    "  kmeans = KMeans(n_clusters = k)\n",
    "  kmeans.cluster_centers_ = centroid.iloc[:k ,:].as_matrix()\n",
    "  kmeans.fit(agr)\n",
    "  inertia.append(kmeans.inertia_)\n",
    "\n",
    "plt.plot([i for i in range(1,16)], inertia, '--.')\n",
    "\n",
    "# listing inertia\n",
    "\n",
    "for i in range(1,16):\n",
    "  print(i, inertia[i-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "d9gWT-xF14uD"
   },
   "source": [
    "##### 1.2.1.3 Mean Shift\n",
    "\n",
    "[ref](https://dashee87.github.io/data%20science/general/Clustering-with-Scikit-with-GIFs/), [ref](http://efavdb.com/mean-shift/) com mais detalhes sobre a regra de atualizacao dos pontos, e [ref](https://spin.atomicobject.com/2015/05/26/mean-shift-clustering/) com aplicacao em image segmentation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "K_GoqIJaMWY_"
   },
   "source": [
    "##### 1.2.1.4 DBSCAN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OFDdmmqSMWqk"
   },
   "source": [
    "[ref](https://medium.com/@agarwalvibhor84/lets-cluster-data-points-using-dbscan-278c5459bee5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yTbPnJUPgQAB"
   },
   "source": [
    "#### 1.2.2 Pattern Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3LZqt5p5gQEq"
   },
   "source": [
    "#### 1.2.3 Dimension Reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tm79708QBIZX"
   },
   "source": [
    "##### 1.2.3.1 t-SNE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uHdWKVk1sn3l"
   },
   "source": [
    "detalhes [[1]](https://mlexplained.com/2018/09/14/paper-dissected-visualizing-data-using-t-sne-explained/), mas a grande forca do t-sne sobre o pca e que nao ha hipoteses de linearidade na tecnica -- portanto t-sne captura tendencias nao-lineares\n",
    "\n",
    "1. para todos os pontos $x_i$\n",
    "  1. dada uma medida de similaridade $\\mathcal{M}(x_i, \\cdot )$ \n",
    "  2. centraliza uma gaussiana no ponto $x_i$\n",
    "  3. projeta cada ponto vizinho na gaussiana $\\rightarrow$ similarity score\n",
    "  4. normaliza os scores $\\rightarrow$ similarity matrix $M_1$\n",
    "2. projeta os pontos em uma dimensao menor\n",
    "3. produz uma nova matriz de similaridade $M_2$, usando dessa vez uma t-distribution\n",
    "4. tenta igualar $M_1$ e $M_2$\n",
    "\n",
    "```\n",
    "[1] t-SNE Explained @ Machine Learning Explained\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ql5A1KGqBfDU"
   },
   "source": [
    "##### 1.2.3.2 LDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tlTpveTUBgRy"
   },
   "source": [
    "[ref](https://sebastianraschka.com/Articles/2014_python_lda.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JFhEEY0sBIlB"
   },
   "source": [
    "##### 1.2.3.3 SVD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gVAJClF6BIsk"
   },
   "source": [
    "##### 1.2.3.4 LSA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "p6G_t0sEBIxj"
   },
   "source": [
    "##### 1.2.3.5 PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2U9VW_nmC2cQ"
   },
   "source": [
    "[ref](https://sebastianraschka.com/Articles/2015_pca_in_3_steps.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5Rp1bHmQf4cQ"
   },
   "source": [
    "## 2 ensemble methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TsKt0jj-A283"
   },
   "source": [
    "em geral ensemble learning e a aprendizagem a partir da forma de uma combinacao linear de modelos base $$f(y|x,\\pi) = \\sum_{m \\in \\mathcal{M}} w_m f_m(y|x)$$\n",
    "\n",
    "1. ha uma relacao funcional bem clara entre ensemble learning e adaptive basis-function learning, portanto **boosting tambem podem ser pensandos como ensemble learning** em que os pesos sao determinados sequencialmente, uma vez que boosting podem ser pensados como uma forma de fittar adaptives basis-function models de forma greedy. \n",
    "\n",
    "$$f(x) = w_0 + \\sum_m^M w_m \\phi_m (x)$$\n",
    "\n",
    "2. ha uma relacao aparente entre ensemble learning e **neural networks**, em que $f_m$ e analogo a m-esima camada oculta e $w_m$ sao os pesos das camadas output.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2UZofUAblsjV"
   },
   "source": [
    "### Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PToWcpkBlwIh"
   },
   "source": [
    "### Stacking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XR7wOXu0lygJ"
   },
   "source": [
    "### Bagging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dkcwBmt4f42q"
   },
   "source": [
    "## 3 reinforcement learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vuDyQ39Tf5Qa"
   },
   "source": [
    "## 4 deep learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Xwsssn52DX0E"
   },
   "source": [
    "[ref](https://sebastianraschka.com/Articles/2015_singlelayer_neurons.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jThrEQXff5Yq"
   },
   "source": [
    "- - -\n",
    "\n",
    "# misc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "o9UIk1QJuQrq"
   },
   "source": [
    "## bias-variance tradeoff\n",
    "\n",
    "poor person's version, partindo do MSE, identificando $\\hat{\\theta} = \\hat{\\theta}(\\mathcal{D})$ a estimativa de parametro resultante da aplicacao do estimador sobre o conjunto de dados, $\\bar{\\theta}=E[\\hat{\\theta}]$ o valor esperado da estimativa -- frequentistamente falando, o valor esperado ao se aplicar o estimador em diferentes conjuntos $\\mathcal{D}$ -- e $\\theta^*$ o valor verdadeiro do parametro\n",
    "\n",
    "antes de mais nada sempre bom lembrar que\n",
    "\n",
    "+ $<x> = \\sum_k kP_k$\n",
    "+ $<x^2> = \\sum_k k^2 P_k$\n",
    "+ $var(x) = <x^2> - <x>^2 = <(x - <x>)^2>$\n",
    "\n",
    "entao uma forma de reproduzir a famigerada decomposicao do erro e\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "MSE =& E[(\\hat{\\theta} - \\theta^*)^2] \\\\\n",
    "= & E[( \\hat{\\theta}- \\bar{\\theta} + \\bar{\\theta} - \\theta^*)^2] \\qquad \\text{adiciona zero na coisa, trick is for kids}\\\\\n",
    "= & E[( \\hat{\\theta}- \\bar{\\theta})^2] + E[2 ( \\hat{\\theta}- \\bar{\\theta})(\\bar{\\theta} - \\theta^*)] + E[(\\bar{\\theta} - \\theta^*)^2]  \\qquad \\text{linearidade}\\\\\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1kdE7S0f01ap"
   },
   "source": [
    "em que\n",
    "\n",
    "+ $E[( \\hat{\\theta}- \\bar{\\theta})^2]$ ok, **variancia do estimador** -- seguindo a mesma ideia com a qual $\\bar{\\theta}$ e definida, ou seja, a variancia do estimador quando aplicado em diferentes conjuntos $\\mathcal{D}$\n",
    "+ $E[2 ( \\hat{\\theta}- \\bar{\\theta})(\\bar{\\theta} - \\theta^*)]$ e zero uma vez que $E[( \\hat{\\theta}- \\bar{\\theta})]=(E[\\hat{\\theta}] - E[\\bar{\\theta}])=(\\bar{\\theta}-\\bar{\\theta})=0$\n",
    "+ $E[(\\bar{\\theta} - \\theta^*)^2] = (\\bar{\\theta} - \\theta^*)^2$ ok, diferenca entre valor esperado do estimador e valor verdadeiro, **bias$^2$** -- que matematicamente nao passa do valor esperado da subtracao de dois numeros, ou seja, nenhum dos termos se trata de uma variavel aleatoria\n",
    "\n",
    "filosoficamente \n",
    "\n",
    "+ **bias$^2$**, incapacidade de um modelo capturar a verdadeira relacao entre input e output\n",
    "+ **variance**, e o que parece"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ABNo-72ff680"
   },
   "source": [
    "\n",
    "## outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tSO8v8uOAyp4"
   },
   "source": [
    "alem do que eu ja fiz\n",
    "\n",
    "[ref](https://sebastianraschka.com/Articles/2014_dixon_test.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cH37167Yf7Lw"
   },
   "source": [
    "## validation\n",
    "\n",
    "dado um conjunto de dados $\\mathcal{D}$\n",
    "\n",
    "- $\\mathcal{T}$ treino (0.8 * $\\mathcal{D}$)\n",
    "- $\\mathcal{V}$ validacao, ou _holdout validation_ (0.2 * $\\mathcal{T}$)\n",
    "- $\\mathcal{T'}$ teste (0.2 * $\\mathcal{D}$)\n",
    "\n",
    "$\\mathcal{V}$ pode ser usado para avaliar varios modelos candidatos\n",
    "\n",
    "1. $M_1, M_2, M_3 \\rightarrow \\mathcal{T}$\n",
    "2. $M_1, M_2, M_3 \\rightarrow \\mathcal{V} \\Rightarrow M^{*}$ \n",
    "3. $M^{*} \\rightarrow \\mathcal{T'}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "j2wQgpxUf7Vb"
   },
   "source": [
    "### cross-validation\n",
    "\n",
    "modelo complexo $\\to$ ruido capturado  $\\to$ overfitting\n",
    "\n",
    "[ref](https://sebastianraschka.com/Articles/2014_intro_supervised_learning.html#cross-validation)\n",
    "[ref](https://sebastianraschka.com/blog/2016/model-evaluation-selection-part3.html#introduction-to-k-fold-cross-validation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Gj6gDVsZf7gk"
   },
   "source": [
    "## cost vs loss vs objective"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1_-GbTpG02PR"
   },
   "source": [
    "+ **loss function** is usually a function **defined on a data point**, prediction and label, and measures the penalty <br/> ex, square loss @ linear regression.\n",
    "\n",
    "+ **cost function** is usually more general, it might be a **sum of loss functions over your training** set plus some model complexity penalty (regularization)<br/> ex, mse @ linear regression\n",
    "\n",
    "+ **objective function** is the most general term for **any function that you optimize during training**<br/> ex, mle @ linear regression \n",
    "\n",
    "[ref](https://stats.stackexchange.com/questions/179026/objective-function-cost-function-loss-function-are-they-the-same-thing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "deoTThFSf790"
   },
   "source": [
    "## distance metrics\n",
    "\n",
    "![](https://raw.githubusercontent.com/thiagodsd/thiagodsd.github.io/master/img/index.png)\n",
    "\n",
    "+ chebyshev\n",
    "+ [hamming distance](https://en.wikipedia.org/wiki/Hamming_distance)\n",
    "+ [levenshtein distance](https://en.wikipedia.org/wiki/Levenshtein_distance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "448WxDuwSFwm"
   },
   "source": [
    "## performance metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "S7n2LWutXOVl"
   },
   "source": [
    "Identificando, modelo $h$ tal que $h(x) = \\hat{y}$ e a predicao para $y$ dado $x$.\n",
    "\n",
    "[ref](https://www.analyticsvidhya.com/blog/2019/08/11-important-model-evaluation-error-metrics/)\n",
    "\n",
    "[ref](https://towardsdatascience.com/20-popular-machine-learning-metrics-part-1-classification-regression-evaluation-metrics-1ca3e282a2ce)\n",
    "\n",
    "[ref](https://medium.com/usf-msds/choosing-the-right-metric-for-machine-learning-models-part-1-a99d7d7414e4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_MxSOZISVSlD"
   },
   "source": [
    "### regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "J2KDdBbqXIDM"
   },
   "source": [
    "* MAE, mean absolute error $$\\frac{1}{N}\\sum_i^N |y_i - h(x)|$$\n",
    "\n",
    "* MSE, mean squared error $$\\frac{1}{N}\\sum_i^N (y_i - h(x))^2$$\n",
    "\n",
    "* RMSE, root mean squared error $$\\sqrt{\\frac{1}{N}\\sum_i^N (y_i - h(x))^2}=\\sqrt{\\text{MSE}}$$\n",
    "\n",
    "* $R^2$, coefficient of determination $$1 - \\frac{\\sum (y_i - h(x))^2}{\\sum(y_i  - \\bar{y})^2}$$\n",
    "\n",
    "* $\\bar{R}^2$, adjusted coefficient of determination, para $k$ features e $n$ linhas $$1 - (1-R^2)\\frac{n-1}{n-(k+1)}$$\n",
    "\n",
    "<br/>\n",
    "\n",
    "como sempre -- devido $\\ell_1$ vs $\\ell_2$ -- MAE melhor que RMSE caso o conjunto tenha muitos outliers, por outro lado MAE envolve calculo de modulo, que geralmente e caro\n",
    "\n",
    "particularidade da RMSE, ela e sensivel a variancia da distribuicao de magnitude dos erros -- o que e diferente de ser sensivel a variancia dos erros, portanto RMSE **nao mede apenas o erro**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QvBOwX8EWo6A"
   },
   "source": [
    "### classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "50BH8hzdWqQ0"
   },
   "source": [
    "## hypothesis Tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4w_IQuiq_WO3"
   },
   "source": [
    "[ref](https://sebastianraschka.com/blog/2018/model-evaluation-selection-part4.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7KgWEknwAYGW"
   },
   "source": [
    "## scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gJc5AhwtAcR8"
   },
   "source": [
    "[ref](https://sebastianraschka.com/Articles/2014_about_feature_scaling.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-4V0H8lqKy8R"
   },
   "source": [
    "## multiclass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yK2F5s2oK2zr"
   },
   "source": [
    "[ref](https://scikit-learn.org/stable/modules/multiclass.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "t0mgqVSOtJOo"
   },
   "source": [
    "## complexity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gXq2NObatM9N"
   },
   "source": [
    "[Computational complexity of machine learning algorithms â€“ The Kernel Trip](https://www.thekerneltrip.com/machine/learning/computational-complexity-learning-algorithms/)\n",
    "\n",
    "[Computational Complexity Theory in ML modeling - Augustine Chang - Medium](https://medium.com/@augustinechang/computational-complexity-theory-in-ml-modeling-66b5a5fa610f)\n",
    "\n",
    "[references - Run-time analysis of common machine learning algorithms - Cross Validated](https://stats.stackexchange.com/questions/19409/run-time-analysis-of-common-machine-learning-algorithms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5ozt7EUGcJuz"
   },
   "source": [
    "# main references"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "s81HX12BcMOc"
   },
   "source": [
    "```\n",
    "@misc{mlcourse50:online,\n",
    "author       = {},\n",
    "title        = {mlcourse.ai | Kaggle},\n",
    "howpublished = {\\url{https://www.kaggle.com/kashnitsky/mlcourse}},\n",
    "month        = {},\n",
    "year         = {},\n",
    "note         = {(Accessed on 01/03/2020)}\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zIT62xLIxs-P"
   },
   "source": [
    "```\n",
    "@misc{DrSebast36:online,\n",
    "author       = {},\n",
    "title        = {Dr. Sebastian Raschka},\n",
    "howpublished = {\\url{https://sebastianraschka.com/}},\n",
    "month        = {},\n",
    "year         = {},\n",
    "note         = {(Accessed on 01/03/2020)}\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JBbrv-gr_YuB"
   },
   "source": [
    "."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "revisao-e-snippets.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
