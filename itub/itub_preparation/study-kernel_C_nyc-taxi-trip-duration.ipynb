{"cells":[{"metadata":{},"cell_type":"markdown","source":"After my, let's say, [_spontaneous_ attempt](https://www.kaggle.com/thiagodsd/nyc-taxi-trip-duration) to find out the duration of taxi trips in New York - which occurs to be my submission ever -, I devote this notebook to study alternative regression approaches and techniques. \n\nAgain, the purpose of this notebook is to study, therefore there are none original idea/solution/approach below, just reproductions and minor adaptations from other sources. Still I'd glad if it helps somebody somewhat.\n\n- - -\n\nReferences first to give an overview:\n\n1. [Outlier Detection Practice: uni/multivariate | Kaggle](https://www.kaggle.com/kevinarvai/outlier-detection-practice-uni-multivariate) - since data cleaning was a step which stuck me a little. Thanks to [Kevin Arvai](https://www.kaggle.com/kevinarvai).\n2. [7 Regression Types and Techniques in Data Science](https://www.analyticsvidhya.com/blog/2015/08/comprehensive-guide-regression/)\n    - [Neural Network Model for House Prices (Keras) | Kaggle](https://www.kaggle.com/diegosiebra/neural-network-model-for-house-prices-keras)\n3. [Blending of 6 Models (Top 10%) | Kaggle](https://www.kaggle.com/sandeepkumar121995/blending-of-6-models-top-10) - thanks to [Sandeep Kumar](https://www.kaggle.com/sandeepkumar121995).\n\n```\n[From EDA to the Top (LB 0.367) | Kaggle](https://www.kaggle.com/gaborfodor/from-eda-to-the-top-lb-0-367)\n[Predicting House Prices || Regression Techniques | Kaggle](https://www.kaggle.com/janiobachmann/predicting-house-prices-regression-techniques)\n[EDA, Introduction to Ensemble Regression | Kaggle](https://www.kaggle.com/yassineghouzam/eda-introduction-to-ensemble-regression)\n```\n\n- - -\n"},{"metadata":{},"cell_type":"markdown","source":"Checking files:"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy  as np\nimport pandas as pd\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","execution_count":58,"outputs":[{"output_type":"stream","text":"/kaggle/input/nyc-taxi-trip-duration/train.csv\n/kaggle/input/nyc-taxi-trip-duration/sample_submission.csv\n/kaggle/input/nyc-taxi-trip-duration/test.csv\n","name":"stdout"}]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","collapsed":true,"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":false},"cell_type":"markdown","source":"Summoning some libs:"},{"metadata":{"trusted":true},"cell_type":"code","source":"import datetime\nimport warnings\nimport pickle\nimport gc\n\n%matplotlib inline\nimport matplotlib.pyplot as plt\nimport seaborn           as sns\nfrom pandas.plotting import scatter_matrix\n\nfrom sklearn import metrics\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.impute          import SimpleImputer\nfrom sklearn.compose         import ColumnTransformer\nfrom sklearn.preprocessing   import OrdinalEncoder, OneHotEncoder\nfrom sklearn.pipeline        import Pipeline\nfrom sklearn.preprocessing   import StandardScaler\n\nfrom sklearn.cluster      import KMeans, DBSCAN\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.tree         import DecisionTreeRegressor\n\nfrom sklearn.metrics import mean_absolute_error, mean_squared_log_error\n\n\nfrom IPython.display import display, FileLink\n\n#\n\nwarnings.filterwarnings('ignore')\nplt.rcParams['figure.figsize'] = [13, 7]\nnp.random.seed(1642)","execution_count":69,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"defining some functions:"},{"metadata":{"trusted":true},"cell_type":"code","source":"def var_cleaner(s):\n    \"\"\"\n    ('var1, var2, ..., varN') -> None\n    \"\"\"\n    trash = list()\n    miss  = list()\n    for v in s.replace(' ', '').split(','):\n        if v in globals():\n            del globals()[v]\n            trash.append(v)\n        else:\n            miss.append(v)\n    print('- DELETED:     {}'.format( ', '.join(trash) ))\n    print('- NOT DEFINED: {}'.format( ', '.join(miss) ))","execution_count":60,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from math import sin, cos, sqrt, atan2, radians\ndef lat_lon_converter(lat1, lon1, lat2, lon2, unit):\n    \"\"\"\n    ref: https://stackoverflow.com/questions/19412462/getting-distance-between-two-points-based-on-latitude-longitude\n    \"\"\"\n    try:\n        R = 6373.0\n        dlon = radians(lon2) - radians(lon1)\n        dlat = radians(lat2) - radians(lat1)\n        a = sin(dlat / 2)**2 + cos(lat1) * cos(lat2) * sin(dlon / 2)**2\n        c = 2 * atan2(sqrt(a), sqrt(1 - a))\n        distance = R * c\n\n        if unit == 'm':\n            return distance * 10e3\n        elif unit == 'km':\n            return distance\n    except ValueError:\n        return np.nan","execution_count":61,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def dbscan_predict(model, X):\n    \"\"\"\n    ref: https://stackoverflow.com/questions/27822752/scikit-learn-predicting-new-points-with-dbscan\n    \"\"\"\n    nr_samples = X.shape[0]\n\n    y_new = np.ones(shape=nr_samples, dtype=int) * -1\n\n    for i in range(nr_samples):\n        diff = model.components_ - X[i, :]   # NumPy broadcasting\n        dist = np.linalg.norm(diff, axis=1)  # Euclidean distance\n        shortest_dist_idx = np.argmin(dist)\n\n        if dist[shortest_dist_idx] < model.eps:\n            y_new[i] = model.labels_[model.core_sample_indices_[shortest_dist_idx]]\n\n    return y_new","execution_count":62,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Loading data:"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train = pd.read_csv('/kaggle/input/nyc-taxi-trip-duration/train.csv')\ndf_test  = pd.read_csv('/kaggle/input/nyc-taxi-trip-duration/test.csv')\n\nprint('train: ', df_train.shape)\nprint('test:  ', df_test.shape)\n\ndisplay( df_train.head() )\ndisplay( df_test.head() )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"_TARGET      = 'trip_duration'\n_NON_FEATURE = set(df_train.columns) - set(df_test.columns)\n_FEATURES    = set(df_train.columns).intersection(set(df_test.columns)) - set(['id'])\n\ndisplay(_FEATURES)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = df_train[_FEATURES]\ntest  = df_test[_FEATURES]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.describe().apply( lambda s: s.apply( lambda x: format(x, '.3f') ) )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sample = train[['dropoff_latitude', 'dropoff_longitude']].sample(10000)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Outlier Detection\n\n![](https://2b1ohome.files.wordpress.com/2019/10/icon_outlier-4.png)"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.hist(sample['dropoff_latitude'], bins=100);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Univariate"},{"metadata":{},"cell_type":"markdown","source":"### Standard Deviation & Interquartile Range Method\n\n> A good statistic for summarizing a non-Gaussian distribution sample of data is the Interquartile Range, or IQR for short."},{"metadata":{"trusted":true},"cell_type":"code","source":"def out_std(data, n_std=3.0, return_thresholds=False):\n    \"\"\"\n    ref: https://www.kaggle.com/kevinarvai/outlier-detection-practice-uni-multivariate#Parametric-methods:-Univariate\n    \"\"\"\n    mean, std    = data.mean(), data.std()\n    cutoff       = std * n_std\n    lower, upper = mean - cutoff, mean + cutoff\n    if return_thresholds:\n        return lower, upper\n    return [True if i < lower or i > upper else False for i in data]\n\ndef out_iqr(data, k=1.5, return_thresholds=False):\n    \"\"\"\n    ref: https://www.kaggle.com/kevinarvai/outlier-detection-practice-uni-multivariate#Parametric-methods:-Univariate\n    \"\"\"\n    q25, q75     = np.percentile(data, 25), np.percentile(data, 75)\n    iqr          = q75 - q25\n    cutoff       = iqr * k\n    lower, upper = q25 - cutoff, q75 + cutoff\n    if return_thresholds:\n        return lower, upper\n    return [True if i < lower or i > upper else False for i in data]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"out_mask_std = out_std(sample['dropoff_latitude'], n_std=3.0)\ndisplay( np.unique(out_mask_std, return_counts=True) )\n\nout_mask_iqr = out_iqr(sample['dropoff_latitude'], k=1.5)\ndisplay( np.unique(out_mask_iqr, return_counts=True) )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"_filter = sample.dtypes[~(sample.dtypes.isin([ np.dtype('object'),  np.dtype('<M8[ns]')]))].index\n\nout_mask_std = sample[_filter].apply(out_std, n_std=3.0)\nout_mask_iqr = sample[_filter].apply(out_iqr, k=1.5)\n\nf, ((ax1, ax2)) = plt.subplots(ncols=2, nrows=1)\n\nsns.heatmap(out_mask_std, cmap='binary', ax=ax1)\nsns.heatmap(out_mask_iqr, cmap='binary', ax=ax2)\n\nax1.set_xticklabels(ax1.get_xticklabels(), rotation=90)\nax1.axes.get_yaxis().set_visible(False)\nax2.set_xticklabels(ax2.get_xticklabels(), rotation=90)\nax2.axes.get_yaxis().set_visible(False)\n\nax1.set_title(r'out_std')\nax2.set_title(r'out_iqr');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Isolation Forest"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import IsolationForest\n\nout_if = IsolationForest(n_estimators=100)\nout_if.fit(sample['dropoff_latitude'].values.reshape(-1, 1))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"xx = np.linspace(sample['dropoff_latitude'].min(), sample['dropoff_latitude'].max(), int(sample.shape[0]/5.0)).reshape(-1, 1)\n\nanomaly_score = out_if.decision_function(xx)\nout_bounds    = out_if.predict(xx)\n\nplt.plot(xx, anomaly_score, label='anomaly score')\nplt.fill_between(xx.T[0], np.min(anomaly_score),  np.max(anomaly_score), where=(out_bounds==-1), alpha=.25, color='r', label='outlier region')\n\nplt.legend();","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Multivariate"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.scatter(sample['dropoff_latitude'], sample['dropoff_longitude'], alpha=0.45);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Isolation Forest"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import IsolationForest\n\nout_if = IsolationForest()\nout_if.fit(sample[['dropoff_latitude', 'dropoff_longitude']])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"xx_bounds = [sample['dropoff_latitude'].min(), sample['dropoff_latitude'].max()]\nyy_bounds = [sample['dropoff_longitude'].min(), sample['dropoff_longitude'].max()]\n\nxx, yy = np.meshgrid(np.linspace(min(xx_bounds), max(xx_bounds), 250),\n                    np.linspace(min(yy_bounds), max(yy_bounds), 250))\n\nzz = out_if.predict(np.c_[xx.ravel(), yy.ravel()])\nzz = zz.reshape(xx.shape)\n\nplt.scatter(sample['dropoff_latitude'], sample['dropoff_longitude'], alpha=0.45);\nplt.contour(xx, yy, zz, levels=[0], colors='black');","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"### Elliptic Envelope\n\n> Outlier detection from covariance estimation may break or not perform well in high-dimensional settings. In particular, one will always take care to work with n_samples > n_features ** 2.\n\n([ref](https://scikit-learn.org/stable/modules/generated/sklearn.covariance.EllipticEnvelope.html#sklearn.covariance.EllipticEnvelope))"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.covariance import EllipticEnvelope\n\nout_elp = EllipticEnvelope()\nout_elp.fit(sample[['dropoff_latitude', 'dropoff_longitude']])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"zz = out_elp.decision_function(np.c_[xx.ravel(), yy.ravel()])\nzz = zz.reshape(xx.shape)\n\nplt.scatter(sample['dropoff_latitude'], sample['dropoff_longitude'], alpha=0.45);\nplt.contour(xx, yy, zz, levels=[0], colors='black');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### DBSCAN"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing  import StandardScaler\nfrom sklearn.cluster        import DBSCAN\n\nX      = StandardScaler().fit_transform(sample.values)\nout_db = DBSCAN(eps=0.75, min_samples=10).fit(X)\nlabels = out_db.labels_\n\nnp.unique(labels, return_counts=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"unique_labels = set(labels)\n\nfor label in unique_labels:\n    sample_mask = [True if l == label else False for l in labels]\n    plt.plot(sample['dropoff_latitude'][sample_mask], sample['dropoff_longitude'][sample_mask], 'o', label=label);\nplt.legend();","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Local Outlier Factor\n\n>  The advantage of sklearn.neighbors.LocalOutlierFactor over the other estimators is shown for the third data set, where the two modes have different densities. This advantage is explained by the local aspect of LOF, meaning that it only compares the score of abnormality of one sample with the scores of its neighbors."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.neighbors import LocalOutlierFactor\n\nout_lof = LocalOutlierFactor(n_neighbors=10, novelty=True)\nout_lof.fit(sample[['dropoff_latitude', 'dropoff_longitude']])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"zz = out_lof.decision_function(np.c_[xx.ravel(), yy.ravel()])\nzz = zz.reshape(xx.shape)\n\nplt.scatter(sample['dropoff_latitude'], sample['dropoff_longitude'], alpha=0.45);\nplt.contour(xx, yy, zz, levels=[0], colors='black');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Feature Engineering & Data Imputing\n\n![](https://2b1ohome.files.wordpress.com/2019/10/icon_cleaning-3.png)\n\nApparently the _NYC Taxi Trip Duration_ dataset doesn't demand complex imputation techniques, so I'll replicate data preparation steps from my original notebook."},{"metadata":{},"cell_type":"markdown","source":"Prophylactic reloadings."},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train = pd.read_csv('/kaggle/input/nyc-taxi-trip-duration/train.csv')\ndf_test  = pd.read_csv('/kaggle/input/nyc-taxi-trip-duration/test.csv')\n\nprint('train: ', df_train.shape)\nprint('test:  ', df_test.shape)\n#display( df_train.head() )\n#display( df_test.head() )\n\n_TARGET      = 'trip_duration'\n_NON_FEATURE = set(df_train.columns) - set(df_test.columns)\n_FEATURES    = set(df_train.columns).intersection(set(df_test.columns)) - set(['id'])\n#display(_FEATURES)\n\ntrain = df_train[_FEATURES]\ntest  = df_test[_FEATURES]","execution_count":57,"outputs":[{"output_type":"stream","text":"train:  (1458644, 11)\ntest:   (625134, 9)\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"Temporal features"},{"metadata":{"trusted":true},"cell_type":"code","source":"train['pickup_dt'] = pd.to_datetime(train['pickup_datetime'], format='%Y-%m-%d %H:%M:%S', errors='ignore')\ntrain = train.drop(columns=['pickup_datetime'])\n\ntrain['pick_minute']     = train['pickup_dt'].dt.minute\ntrain['pick_hour']       = train['pickup_dt'].dt.hour\ntrain['pick_day']        = train['pickup_dt'].dt.day\ntrain['pick_month']      = train['pickup_dt'].dt.month\ntrain['pick_year']       = train['pickup_dt'].dt.year\ntrain['pick_quarter']    = train['pickup_dt'].dt.quarter\ntrain['pick_weekofyear'] = train['pickup_dt'].dt.weekofyear","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Spatial features"},{"metadata":{"trusted":true},"cell_type":"code","source":"train['lon_lat_manhattan']    = abs(train['dropoff_longitude']-train['pickup_longitude']) + abs(train['dropoff_latitude']-train['pickup_latitude'])\ntrain['dist_manhattan_meter'] = train.apply( lambda x: lat_lon_converter(x['pickup_latitude'], \n                                                                         x['pickup_longitude'],\n                                                                         x['dropoff_latitude'], \n                                                                         x['dropoff_longitude'],\n                                                                         'm'), axis=1 )","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Clustering location"},{"metadata":{"trusted":true},"cell_type":"code","source":"sample = train[['pickup_latitude', 'pickup_longitude', 'dropoff_latitude', 'dropoff_longitude']].sample(10000, random_state=37)\nX      = StandardScaler().fit_transform(sample.values)","execution_count":65,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"db     = DBSCAN(eps=0.75, min_samples=10).fit(X)\nkmeans = KMeans(n_clusters=5, random_state=37).fit(X)","execution_count":66,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['db_predict']     = dbscan_predict(db, StandardScaler().fit_transform(train[['pickup_latitude', \n                                                                                   'pickup_longitude', \n                                                                                   'dropoff_latitude', \n                                                                                   'dropoff_longitude']].values))\ntrain['kmeans_predict'] = kmeans.predict(StandardScaler().fit_transform(train[['pickup_latitude', \n                                                                               'pickup_longitude', \n                                                                               'dropoff_latitude', \n                                                                               'dropoff_longitude']].values))","execution_count":68,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"Preparing to regression"},{"metadata":{"trusted":true},"cell_type":"code","source":"_FILTERS = {\n            'int'   : [ [None], [np.dtype('int64')] ],\n            'float' : [ [None], [np.dtype('float64')] ],\n            'cat'   : [ [None], [np.dtype('object')] ],\n            'date'  : [ [None], [np.dtype('<M8[ns]')] ]\n           }\n\nfor k in _FILTERS:\n    _FILTERS[k][0] = set(train.dtypes[ train.dtypes.isin(_FILTERS[k][1]) ].index.to_list())\n    print( k, _FILTERS[k][0] )\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.impute          import SimpleImputer\nfrom sklearn.compose         import ColumnTransformer\nfrom sklearn.preprocessing   import OrdinalEncoder, OneHotEncoder\nfrom sklearn.pipeline        import Pipeline\nfrom sklearn.preprocessing   import StandardScaler\n\nint_pipeline   = Pipeline([ ('imputer', SimpleImputer(strategy=\"constant\", fill_value=-1)) ])\nfloat_pipeline = Pipeline([ \n                          ('imputer'   , SimpleImputer(strategy=\"median\")),\n                          ('std_scaler', StandardScaler())\n                          ])\n\nfull_pipeline = ColumnTransformer([\n                                  ('int'  , int_pipeline,    list(_FILTERS['int'][0]) ),\n                                  ('float', float_pipeline,  list(_FILTERS['float'][0]) ) ,\n                                  ('cat'  , OneHotEncoder(), list(_FILTERS['cat'][0]) )\n                                  ])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_x = test[ _FILTER_INT.union(_FILTER_FLOAT).union(_FILTER_CAT) ]\ntest_prepared = full_pipeline.fit_transform(test_x)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}